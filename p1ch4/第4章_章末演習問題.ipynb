{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WehYXzWpwhi1"},"source":["第4章の章末演習問題"]},{"cell_type":"markdown","metadata":{"id":"lRoPRKcMwlbZ"},"source":["### [1] 携帯電話やデジタルカメラで、赤、青、緑の物体の写真を何枚か撮りましょう（カメラがない場合は、インターネットからダウンロードすることもできます）。\n","※ ここではGoogle Colaraboratoryでの実行を想定しています。\n"]},{"cell_type":"markdown","metadata":{"id":"YnFAcuysEQDq"},"source":["※本フォルダに、無料写真素材　写真AC（商用利用可）のデータを配置しています。\n","\n","apple.jpg、sky.jpg、forest.jpg\n","\n","\n","https://www.photo-ac.com/main/detail/345453?title=%E3%83%AA%E3%83%B3%E3%82%B4&searchId=68658080\n","\n","https://www.photo-ac.com/main/detail/3343123?title=%E3%81%99%E3%81%A3%E3%81%8D%E3%82%8A%E6%99%B4%E3%82%8C%E3%82%84%E3%81%8B%E3%81%AA%E5%A4%8F%E3%81%AE%E7%A9%BA&searchId=68660407\n","\n","https://www.photo-ac.com/main/detail/3518379?title=%E6%A8%B9%E6%9C%A8_%E6%A3%AE%E6%9E%97_3&searchId=68662146\n","\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/MyDrive/MyStudy/dlwpt-code-ja/p1ch4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ROz66CsKhbUx","executionInfo":{"status":"ok","timestamp":1690676160398,"user_tz":-540,"elapsed":23615,"user":{"displayName":"Takafumi Matsuyama","userId":"04838575372759959178"}},"outputId":"6006c3a3-9ad0-45bf-8053-c2efc269f4a4"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/MyStudy/dlwpt-code-ja/p1ch4\n"]}]},{"cell_type":"code","source":["%pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"mENIz6viiWea","executionInfo":{"status":"ok","timestamp":1690676160400,"user_tz":-540,"elapsed":13,"user":{"displayName":"Takafumi Matsuyama","userId":"04838575372759959178"}},"outputId":"b1bb8a93-266d-4b15-a685-99e59dc77ea9"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/MyStudy/dlwpt-code-ja/p1ch4'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"FSZC8BAJw9A-"},"source":["#### （a）各画像を読み込み、テンソルに変換してください。"]},{"cell_type":"code","metadata":{"id":"kduB0LXvxL0b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690676167118,"user_tz":-540,"elapsed":6725,"user":{"displayName":"Takafumi Matsuyama","userId":"04838575372759959178"}},"outputId":"09992e5a-eeda-4ba4-c5f4-4d984905aea1"},"source":["# 回答\n","import torch\n","import imageio\n","\n","apple = imageio.imread('fig/apple.jpg')\n","apple.shape"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-3-686cf9f7ea3d>:5: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n","  apple = imageio.imread('fig/apple.jpg')\n"]},{"output_type":"execute_result","data":{"text/plain":["(540, 960, 3)"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["apple_t = torch.from_numpy(apple)\n","apple_t.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"17gt1CrBjzNs","executionInfo":{"status":"ok","timestamp":1690676167118,"user_tz":-540,"elapsed":9,"user":{"displayName":"Takafumi Matsuyama","userId":"04838575372759959178"}},"outputId":"f105ff12-5dbf-48c7-926e-7d5eb945951f"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([540, 960, 3])"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["def convert_jpg_2_tensor(filename):\n","    img = imageio.imread(filename)\n","    img_t = torch.from_numpy(img)\n","    img_t = img_t.permute(2, 0, 1)\n","    img_t = img_t[:3].float()\n","    return img_t"],"metadata":{"id":"M7zkNHl5pL9N","executionInfo":{"status":"ok","timestamp":1690676167119,"user_tz":-540,"elapsed":7,"user":{"displayName":"Takafumi Matsuyama","userId":"04838575372759959178"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["img_apple_t = convert_jpg_2_tensor('fig/apple.jpg')\n","img_forest_t = convert_jpg_2_tensor('fig/forest.jpg')\n","img_sky_t = convert_jpg_2_tensor('fig/sky.jpg')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pS8c7PLco-mP","executionInfo":{"status":"ok","timestamp":1690676168135,"user_tz":-540,"elapsed":1022,"user":{"displayName":"Takafumi Matsuyama","userId":"04838575372759959178"}},"outputId":"4d6419e0-c0c3-47ba-dabd-cd4033715df6"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-bcf98f8a3c81>:2: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n","  img = imageio.imread(filename)\n"]}]},{"cell_type":"markdown","metadata":{"id":"YO24ioODyKtO"},"source":["#### (b）各画像テンソルについて、.mean()メソッドを使用して、画像の明るさを求めてください。"]},{"cell_type":"code","metadata":{"id":"U_7dlCScyWYr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690676456637,"user_tz":-540,"elapsed":5,"user":{"displayName":"Takafumi Matsuyama","userId":"04838575372759959178"}},"outputId":"c30d93cc-f80d-4a91-c5f2-7d339945f6ad"},"source":["# 回答\n","print(img_apple_t.mean())\n","print(img_forest_t.mean())\n","print(img_sky_t.mean())"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(124.0309)\n","tensor(87.2000)\n","tensor(126.8331)\n"]}]},{"cell_type":"markdown","metadata":{"id":"y6rfvnbqJloe"},"source":["#### （c）各画像の各チャンネルの平均を取ってください。求めたチャンネルの平均値だけから、赤、緑、青の物体を識別できるか確認してください。"]},{"cell_type":"code","metadata":{"id":"41982_5lCCUL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690676757376,"user_tz":-540,"elapsed":380,"user":{"displayName":"Takafumi Matsuyama","userId":"04838575372759959178"}},"outputId":"4f31307e-b984-4657-e40c-a535719e3a17"},"source":["# 回答\n","print('各チャンネルの平均値')\n","print('林檎の画像: R {:7.2f}, G {:7.2f}, B {:7.2f}'.format(*img_apple_t.mean(dim=1).mean(dim=1)))\n","print('森林の画像: R {:7.2f}, G {:7.2f}, B {:7.2f}'.format(*img_forest_t.mean(dim=1).mean(dim=1)))\n","print('青空の画像: R {:7.2f}, G {:7.2f}, B {:7.2f}'.format(*img_sky_t.mean(dim=1).mean(dim=1)))"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["各チャンネルの平均値\n","林檎の画像: R  190.31, G  103.50, B   78.28\n","森林の画像: R   82.50, G  112.32, B   66.78\n","青空の画像: R   64.19, G  125.72, B  190.59\n"]}]},{"cell_type":"markdown","metadata":{"id":"EdWttSfHzGKz"},"source":["### [2] Pythonのソースコードを含む比較的大きなファイルを用意してください。\n","\n","※https://github.com/YutaroOgawa/pytorch_advanced/blob/master/1_image_classification/utils/dataloader_image_classification.py\n","\n","をフォルダ内に用意しています。\n"]},{"cell_type":"markdown","metadata":{"id":"9yzctZuLzODR"},"source":["#### （a）ソースファイル内のすべての単語のインデックスを作成してください (トークン化はシンプルにしても複雑にしても構いません。 最初は正規表現r\"[^a-zA-Z0-9_]+\"とスペースで、単語を一度置き換えることをおすすめします)。"]},{"cell_type":"code","metadata":{"id":"DFq3TaQBy-ZE","colab":{"base_uri":"https://localhost:8080/","height":510},"executionInfo":{"status":"ok","timestamp":1690677339533,"user_tz":-540,"elapsed":537,"user":{"displayName":"Takafumi Matsuyama","userId":"04838575372759959178"}},"outputId":"b554e7f7-9279-4c92-aaa8-26940c0b6795"},"source":["# 回答\n","with open('sample_code.py', encoding='utf8') as f:\n","    text = f.read()\n","text"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'import glob\\nimport os.path as osp\\nimport torch.utils.data as data\\nfrom torchvision import models, transforms\\nfrom PIL import Image\\n\\n\\nclass ImageTransform():\\n    \"\"\"\\n    画像の前処理クラス。訓練時、検証時で異なる動作をする。\\n    画像のサイズをリサイズし、色を標準化する。\\n    訓練時はRandomResizedCropとRandomHorizontalFlipでデータオーギュメンテーションする。\\n\\n\\n    Attributes\\n    ----------\\n    resize : int\\n        リサイズ先の画像の大きさ。\\n    mean : (R, G, B)\\n        各色チャネルの平均値。\\n    std : (R, G, B)\\n        各色チャネルの標準偏差。\\n    \"\"\"\\n\\n    def __init__(self, resize, mean, std):\\n        self.data_transform = {\\n            \\'train\\': transforms.Compose([\\n                transforms.RandomResizedCrop(\\n                    resize, scale=(0.5, 1.0)),  # データオーギュメンテーション\\n                transforms.RandomHorizontalFlip(),  # データオーギュメンテーション\\n                transforms.ToTensor(),  # テンソルに変換\\n                transforms.Normalize(mean, std)  # 標準化\\n            ]),\\n            \\'val\\': transforms.Compose([\\n                transforms.Resize(resize),  # リサイズ\\n                transforms.CenterCrop(resize),  # 画像中央をresize×resizeで切り取り\\n                transforms.ToTensor(),  # テンソルに変換\\n                transforms.Normalize(mean, std)  # 標準化\\n            ])\\n        }\\n\\n    def __call__(self, img, phase=\\'train\\'):\\n        \"\"\"\\n        Parameters\\n        ----------\\n        phase : \\'train\\' or \\'val\\'\\n            前処理のモードを指定。\\n        \"\"\"\\n        return self.data_transform[phase](img)\\n\\n\\ndef make_datapath_list(phase=\"train\"):\\n    \"\"\"\\n    データのパスを格納したリストを作成する。\\n\\n    Parameters\\n    ----------\\n    phase : \\'train\\' or \\'val\\'\\n        訓練データか検証データかを指定する\\n\\n    Returns\\n    -------\\n    path_list : list\\n        データへのパスを格納したリスト\\n    \"\"\"\\n\\n    rootpath = \"./data/hymenoptera_data/\"\\n    target_path = osp.join(rootpath+phase+\\'/**/*.jpg\\')\\n    print(target_path)\\n\\n    path_list = []  # ここに格納する\\n\\n    # globを利用してサブディレクトリまでファイルパスを取得する\\n    for path in glob.glob(target_path):\\n        path_list.append(path)\\n\\n    return path_list\\n\\n\\nclass HymenopteraDataset(data.Dataset):\\n    \"\"\"\\n    アリとハチの画像のDatasetクラス。PyTorchのDatasetクラスを継承。\\n\\n    Attributes\\n    ----------\\n    file_list : リスト\\n        画像のパスを格納したリスト\\n    transform : object\\n        前処理クラスのインスタンス\\n    phase : \\'train\\' or \\'test\\'\\n        学習か訓練かを設定する。\\n    \"\"\"\\n\\n    def __init__(self, file_list, transform=None, phase=\\'train\\'):\\n        self.file_list = file_list  # ファイルパスのリスト\\n        self.transform = transform  # 前処理クラスのインスタンス\\n        self.phase = phase  # train or valの指定\\n\\n    def __len__(self):\\n        \\'\\'\\'画像の枚数を返す\\'\\'\\'\\n        return len(self.file_list)\\n\\n    def __getitem__(self, index):\\n        \\'\\'\\'\\n        前処理をした画像のTensor形式のデータとラベルを取得\\n        \\'\\'\\'\\n\\n        # index番目の画像をロード\\n        img_path = self.file_list[index]\\n        img = Image.open(img_path)  # [高さ][幅][色RGB]\\n\\n        # 画像の前処理を実施\\n        img_transformed = self.transform(\\n            img, self.phase)  # torch.Size([3, 224, 224])\\n\\n        # 画像のラベルをファイル名から抜き出す\\n        if self.phase == \"train\":\\n            label = img_path[30:34]\\n        elif self.phase == \"val\":\\n            label = img_path[28:32]\\n\\n        # ラベルを数値に変更する\\n        if label == \"ants\":\\n            label = 0\\n        elif label == \"bees\":\\n            label = 1\\n\\n        return img_transformed, label\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BtExEhB-STnz","executionInfo":{"status":"ok","timestamp":1690678436529,"user_tz":-540,"elapsed":3,"user":{"displayName":"Takafumi Matsuyama","userId":"04838575372759959178"}},"outputId":"daa8cf83-965c-4772-ec80-741552136ad5"},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3135"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["import re\n","words_in_file = re.findall(r\"[a-zA-Z0-9_]+\", text)\n","\n","print(words_in_file)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p_s8gIsvQRRO","executionInfo":{"status":"ok","timestamp":1690678038231,"user_tz":-540,"elapsed":5,"user":{"displayName":"Takafumi Matsuyama","userId":"04838575372759959178"}},"outputId":"e5d756e0-b378-4441-f97f-85dad21f3288"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["['import', 'glob', 'import', 'os', 'path', 'as', 'osp', 'import', 'torch', 'utils', 'data', 'as', 'data', 'from', 'torchvision', 'import', 'models', 'transforms', 'from', 'PIL', 'import', 'Image', 'class', 'ImageTransform', 'RandomResizedCrop', 'RandomHorizontalFlip', 'Attributes', 'resize', 'int', 'mean', 'R', 'G', 'B', 'std', 'R', 'G', 'B', 'def', '__init__', 'self', 'resize', 'mean', 'std', 'self', 'data_transform', 'train', 'transforms', 'Compose', 'transforms', 'RandomResizedCrop', 'resize', 'scale', '0', '5', '1', '0', 'transforms', 'RandomHorizontalFlip', 'transforms', 'ToTensor', 'transforms', 'Normalize', 'mean', 'std', 'val', 'transforms', 'Compose', 'transforms', 'Resize', 'resize', 'transforms', 'CenterCrop', 'resize', 'resize', 'resize', 'transforms', 'ToTensor', 'transforms', 'Normalize', 'mean', 'std', 'def', '__call__', 'self', 'img', 'phase', 'train', 'Parameters', 'phase', 'train', 'or', 'val', 'return', 'self', 'data_transform', 'phase', 'img', 'def', 'make_datapath_list', 'phase', 'train', 'Parameters', 'phase', 'train', 'or', 'val', 'Returns', 'path_list', 'list', 'rootpath', 'data', 'hymenoptera_data', 'target_path', 'osp', 'join', 'rootpath', 'phase', 'jpg', 'print', 'target_path', 'path_list', 'glob', 'for', 'path', 'in', 'glob', 'glob', 'target_path', 'path_list', 'append', 'path', 'return', 'path_list', 'class', 'HymenopteraDataset', 'data', 'Dataset', 'Dataset', 'PyTorch', 'Dataset', 'Attributes', 'file_list', 'transform', 'object', 'phase', 'train', 'or', 'test', 'def', '__init__', 'self', 'file_list', 'transform', 'None', 'phase', 'train', 'self', 'file_list', 'file_list', 'self', 'transform', 'transform', 'self', 'phase', 'phase', 'train', 'or', 'val', 'def', '__len__', 'self', 'return', 'len', 'self', 'file_list', 'def', '__getitem__', 'self', 'index', 'Tensor', 'index', 'img_path', 'self', 'file_list', 'index', 'img', 'Image', 'open', 'img_path', 'RGB', 'img_transformed', 'self', 'transform', 'img', 'self', 'phase', 'torch', 'Size', '3', '224', '224', 'if', 'self', 'phase', 'train', 'label', 'img_path', '30', '34', 'elif', 'self', 'phase', 'val', 'label', 'img_path', '28', '32', 'if', 'label', 'ants', 'label', '0', 'elif', 'label', 'bees', 'label', '1', 'return', 'img_transformed', 'label']\n"]}]},{"cell_type":"code","source":["len(words_in_file)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gldU1ZvPS24P","executionInfo":{"status":"ok","timestamp":1690678562370,"user_tz":-540,"elapsed":3,"user":{"displayName":"Takafumi Matsuyama","userId":"04838575372759959178"}},"outputId":"fc6949b5-4247-4b9e-9a6e-2bd3959031a1"},"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["230"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["word_list = list(set(words_in_file))\n","word_list = sorted(word_list)\n","len(word_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EyP5EDgDQ-jV","executionInfo":{"status":"ok","timestamp":1690678127317,"user_tz":-540,"elapsed":5,"user":{"displayName":"Takafumi Matsuyama","userId":"04838575372759959178"}},"outputId":"73a59c22-fae1-47cd-e15c-69013f17b679"},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["90"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["word2index_dict = {word: i for i, word in enumerate(word_list)}"],"metadata":{"id":"MqrFNtH_RQ5O","executionInfo":{"status":"ok","timestamp":1690678239514,"user_tz":-540,"elapsed":360,"user":{"displayName":"Takafumi Matsuyama","userId":"04838575372759959178"}}},"execution_count":41,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OBEZN-2gzy-P"},"source":["#### （b）本章の「高慢と偏見」で作ったインデックスと比較してみてください。どちらの方がサイズは大きいですか？"]},{"cell_type":"code","metadata":{"id":"4e-l9C2S5Ihj"},"source":["# 回答"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ei2PjV2b1pVy"},"source":["#### （c）ソースコードファイルのワンホットエンコーディングを作成してください。"]},{"cell_type":"code","metadata":{"id":"6ij9x9461uDC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690678667033,"user_tz":-540,"elapsed":6,"user":{"displayName":"Takafumi Matsuyama","userId":"04838575372759959178"}},"outputId":"266fb30d-e8c8-4f3e-8d3b-674d3e2fa6e4"},"source":["# 回答\n","word_t = torch.zeros(len(words_in_file) ,len(word2index_dict))\n","for i, word in enumerate(words_in_file):\n","    word_index = word2index_dict[word]\n","    word_t[i, word_index] = 1\n","    print('{:2} {:4} {}'.format(i, word_index, word))\n","\n","print(word_t.shape)"],"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":[" 0   54 import\n"," 1   48 glob\n"," 2   54 import\n"," 3   69 os\n"," 4   71 path\n"," 5   38 as\n"," 6   70 osp\n"," 7   54 import\n"," 8   83 torch\n"," 9   88 utils\n","10   41 data\n","11   38 as\n","12   41 data\n","13   47 from\n","14   84 torchvision\n","15   54 import\n","16   65 models\n","17   87 transforms\n","18   47 from\n","19   20 PIL\n","20   54 import\n","21   16 Image\n","22   40 class\n","23   17 ImageTransform\n","24   26 RandomResizedCrop\n","25   25 RandomHorizontalFlip\n","26    9 Attributes\n","27   75 resize\n","28   57 int\n","29   64 mean\n","30   23 R\n","31   14 G\n","32   10 B\n","33   80 std\n","34   23 R\n","35   14 G\n","36   10 B\n","37   43 def\n","38   34 __init__\n","39   79 self\n","40   75 resize\n","41   64 mean\n","42   80 std\n","43   79 self\n","44   42 data_transform\n","45   85 train\n","46   87 transforms\n","47   12 Compose\n","48   87 transforms\n","49   26 RandomResizedCrop\n","50   75 resize\n","51   78 scale\n","52    0 0\n","53    8 5\n","54    1 1\n","55    0 0\n","56   87 transforms\n","57   25 RandomHorizontalFlip\n","58   87 transforms\n","59   31 ToTensor\n","60   87 transforms\n","61   19 Normalize\n","62   64 mean\n","63   80 std\n","64   89 val\n","65   87 transforms\n","66   12 Compose\n","67   87 transforms\n","68   27 Resize\n","69   75 resize\n","70   87 transforms\n","71   11 CenterCrop\n","72   75 resize\n","73   75 resize\n","74   75 resize\n","75   87 transforms\n","76   31 ToTensor\n","77   87 transforms\n","78   19 Normalize\n","79   64 mean\n","80   80 std\n","81   43 def\n","82   32 __call__\n","83   79 self\n","84   51 img\n","85   73 phase\n","86   85 train\n","87   21 Parameters\n","88   73 phase\n","89   85 train\n","90   68 or\n","91   89 val\n","92   76 return\n","93   79 self\n","94   42 data_transform\n","95   73 phase\n","96   51 img\n","97   43 def\n","98   63 make_datapath_list\n","99   73 phase\n","100   85 train\n","101   21 Parameters\n","102   73 phase\n","103   85 train\n","104   68 or\n","105   89 val\n","106   28 Returns\n","107   72 path_list\n","108   62 list\n","109   77 rootpath\n","110   41 data\n","111   49 hymenoptera_data\n","112   81 target_path\n","113   70 osp\n","114   58 join\n","115   77 rootpath\n","116   73 phase\n","117   59 jpg\n","118   74 print\n","119   81 target_path\n","120   72 path_list\n","121   48 glob\n","122   46 for\n","123   71 path\n","124   55 in\n","125   48 glob\n","126   48 glob\n","127   81 target_path\n","128   72 path_list\n","129   37 append\n","130   71 path\n","131   76 return\n","132   72 path_list\n","133   40 class\n","134   15 HymenopteraDataset\n","135   41 data\n","136   13 Dataset\n","137   13 Dataset\n","138   22 PyTorch\n","139   13 Dataset\n","140    9 Attributes\n","141   45 file_list\n","142   86 transform\n","143   66 object\n","144   73 phase\n","145   85 train\n","146   68 or\n","147   82 test\n","148   43 def\n","149   34 __init__\n","150   79 self\n","151   45 file_list\n","152   86 transform\n","153   18 None\n","154   73 phase\n","155   85 train\n","156   79 self\n","157   45 file_list\n","158   45 file_list\n","159   79 self\n","160   86 transform\n","161   86 transform\n","162   79 self\n","163   73 phase\n","164   73 phase\n","165   85 train\n","166   68 or\n","167   89 val\n","168   43 def\n","169   35 __len__\n","170   79 self\n","171   76 return\n","172   61 len\n","173   79 self\n","174   45 file_list\n","175   43 def\n","176   33 __getitem__\n","177   79 self\n","178   56 index\n","179   30 Tensor\n","180   56 index\n","181   52 img_path\n","182   79 self\n","183   45 file_list\n","184   56 index\n","185   51 img\n","186   16 Image\n","187   67 open\n","188   52 img_path\n","189   24 RGB\n","190   53 img_transformed\n","191   79 self\n","192   86 transform\n","193   51 img\n","194   79 self\n","195   73 phase\n","196   83 torch\n","197   29 Size\n","198    4 3\n","199    2 224\n","200    2 224\n","201   50 if\n","202   79 self\n","203   73 phase\n","204   85 train\n","205   60 label\n","206   52 img_path\n","207    5 30\n","208    7 34\n","209   44 elif\n","210   79 self\n","211   73 phase\n","212   89 val\n","213   60 label\n","214   52 img_path\n","215    3 28\n","216    6 32\n","217   50 if\n","218   60 label\n","219   36 ants\n","220   60 label\n","221    0 0\n","222   44 elif\n","223   60 label\n","224   39 bees\n","225   60 label\n","226    1 1\n","227   76 return\n","228   53 img_transformed\n","229   60 label\n","torch.Size([230, 90])\n"]}]},{"cell_type":"code","source":["word_onehot_t = torch.zeros(word_t.shape[0], word_t.shape[1])\n","index_in_file_t = torch.zeros(len(words_in_file))\n","\n","for i, word in enumerate(words_in_file):\n","    word_index = word2index_dict[word]\n","    index_in_file_t[i] = torch.tensor(word_index)\n","\n","index_in_file_t = index_in_file_t.long()\n","print(index_in_file_t)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1x5r3A-BTbd9","executionInfo":{"status":"ok","timestamp":1690678956908,"user_tz":-540,"elapsed":4,"user":{"displayName":"Takafumi Matsuyama","userId":"04838575372759959178"}},"outputId":"b254cb58-0df6-4e22-de9e-de981cf57214"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([54, 48, 54, 69, 71, 38, 70, 54, 83, 88, 41, 38, 41, 47, 84, 54, 65, 87,\n","        47, 20, 54, 16, 40, 17, 26, 25,  9, 75, 57, 64, 23, 14, 10, 80, 23, 14,\n","        10, 43, 34, 79, 75, 64, 80, 79, 42, 85, 87, 12, 87, 26, 75, 78,  0,  8,\n","         1,  0, 87, 25, 87, 31, 87, 19, 64, 80, 89, 87, 12, 87, 27, 75, 87, 11,\n","        75, 75, 75, 87, 31, 87, 19, 64, 80, 43, 32, 79, 51, 73, 85, 21, 73, 85,\n","        68, 89, 76, 79, 42, 73, 51, 43, 63, 73, 85, 21, 73, 85, 68, 89, 28, 72,\n","        62, 77, 41, 49, 81, 70, 58, 77, 73, 59, 74, 81, 72, 48, 46, 71, 55, 48,\n","        48, 81, 72, 37, 71, 76, 72, 40, 15, 41, 13, 13, 22, 13,  9, 45, 86, 66,\n","        73, 85, 68, 82, 43, 34, 79, 45, 86, 18, 73, 85, 79, 45, 45, 79, 86, 86,\n","        79, 73, 73, 85, 68, 89, 43, 35, 79, 76, 61, 79, 45, 43, 33, 79, 56, 30,\n","        56, 52, 79, 45, 56, 51, 16, 67, 52, 24, 53, 79, 86, 51, 79, 73, 83, 29,\n","         4,  2,  2, 50, 79, 73, 85, 60, 52,  5,  7, 44, 79, 73, 89, 60, 52,  3,\n","         6, 50, 60, 36, 60,  0, 44, 60, 39, 60,  1, 76, 53, 60])\n"]}]},{"cell_type":"code","source":["word_onehot_t.scatter_(\n","    dim=1,\n","    index=index_in_file_t.unsqueeze(1),\n","    value=1.0\n",")\n","\n","print(word_onehot_t.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HSQ3Qn8iUdCp","executionInfo":{"status":"ok","timestamp":1690679044753,"user_tz":-540,"elapsed":3,"user":{"displayName":"Takafumi Matsuyama","userId":"04838575372759959178"}},"outputId":"ef22ebb4-4acf-4aaf-e3ba-a8e49bace3ca"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([230, 90])\n"]}]},{"cell_type":"markdown","metadata":{"id":"bT84rW88KnwC"},"source":["#### （d）今回のエンコーディングで失われる情報は何でしょうか？本章での「高慢と偏見」のエンコーディングで失われた情報と比較してみてください。"]},{"cell_type":"code","metadata":{"id":"kgJ_bDJzKrcA"},"source":["# 回答"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xqu60yKps9Rh"},"source":["#　省略"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6RKZsWxPz6kz"},"source":["以上。\n"]}]}